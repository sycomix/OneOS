{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Current Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CWD to parent dir\n",
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/waser/Projets/OneOS/src'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Console.client.chains.models import get_llm, vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = vLLM(max_tokens=8096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could reach LLM: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Could reach LLM: {llm.is_nlp_server_up()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolphin_prompt_template = \"\"\"<|im_start|>system\n",
    "You are Dolphin, a helpful AI assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=5085): Max retries exceeded with url: /generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f856f98be50>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connection.py:200\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    201\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    203\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    204\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    206\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    497\u001b[0m         method,\n\u001b[1;32m    498\u001b[0m         url,\n\u001b[1;32m    499\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    500\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    501\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    502\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    503\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    504\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connection.py:388\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 388\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders()\n\u001b[1;32m    390\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    980\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connection.py:236\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[1;32m    238\u001b[0m         \u001b[39m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connection.py:215\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    216\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f856f98be50>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5085): Max retries exceeded with url: /generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f856f98be50>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/waser/Projets/OneOS/src/notebook/dolphin_mistral.ipynb Cellule 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/waser/Projets/OneOS/src/notebook/dolphin_mistral.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm(dolphin_prompt_template\u001b[39m.\u001b[39;49mformat(prompt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHello World!\u001b[39;49m\u001b[39m\"\u001b[39;49m), stop\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39m<|im_end|>\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m<|im_start|>\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:876\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    870\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    877\u001b[0m         [prompt],\n\u001b[1;32m    878\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    879\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    880\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    881\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    882\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    883\u001b[0m     )\n\u001b[1;32m    884\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    886\u001b[0m )\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    532\u001b[0m                 prompts,\n\u001b[1;32m    533\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    534\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    536\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    537\u001b[0m             )\n\u001b[1;32m    538\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:126\u001b[0m, in \u001b[0;36mvLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39mlast_line)]], llm_output\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mcontent))     \n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(p))] \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m prompts], llm_output\u001b[39m=\u001b[39m{})\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:126\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39mlast_line)]], llm_output\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mcontent))     \n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(p))] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts], llm_output\u001b[39m=\u001b[39m{})\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:94\u001b[0m, in \u001b[0;36mvLLM._call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m     92\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     93\u001b[0m use_beam_search \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost_http_request(prompt, api_url, stop, n, stream, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemperature, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_tokens, use_beam_search)\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_response(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:53\u001b[0m, in \u001b[0;36mvLLM.post_http_request\u001b[0;34m(self, prompt, api_url, stop, n, stream, temperature, max_tokens, use_beam_search)\u001b[0m\n\u001b[1;32m     43\u001b[0m headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient_name}\n\u001b[1;32m     44\u001b[0m pload \u001b[39m=\u001b[39m {\n\u001b[1;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt,\n\u001b[1;32m     46\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop\n\u001b[1;32m     52\u001b[0m }\n\u001b[0;32m---> 53\u001b[0m response \u001b[39m=\u001b[39m post(api_url, headers\u001b[39m=\u001b[39;49mheaders, json\u001b[39m=\u001b[39;49mpload, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5085): Max retries exceeded with url: /generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f856f98be50>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "llm(dolphin_prompt_template.format(prompt=\"Hello World!\"), stop=[\"<|im_end|>\", \"<|im_start|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "__markup_prompt__ = \"\"\"<|im_start|>{key}\n",
    "{value}<|im_end|>\"\"\"\n",
    "__markup_pred__ = \"\"\"<|im_start|>assistant\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Assistant, my personal AI assistant. You are curious, honest, respectful and sometimes witty and sarcastic.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the best way to travel the stars?  Please answer step by step.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__markup_system__ = __markup_prompt__.format(key=\"system\", value=\"You are Assistant, my personal AI assistant. You are curious, honest, respectful and sometimes witty and sarcastic.\")\n",
    "markup_prompt = \"\\n\".join([\n",
    "    __markup_system__,\n",
    "    __markup_prompt__.format(key=\"user\", value=\"What is the best way to travel the stars?  Please answer step by step.\"),\n",
    "    __markup_pred__,\n",
    "])\n",
    "print(markup_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/waser/Projets/OneOS/src/notebook/dolphin_mistral.ipynb Cellule 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/waser/Projets/OneOS/src/notebook/dolphin_mistral.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(llm(markup_prompt, stop\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39m<|im_end|>\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m<|im_start|>\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mstrip())\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:876\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    870\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    877\u001b[0m         [prompt],\n\u001b[1;32m    878\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    879\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    880\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    881\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    882\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    883\u001b[0m     )\n\u001b[1;32m    884\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    886\u001b[0m )\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    532\u001b[0m                 prompts,\n\u001b[1;32m    533\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    534\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    536\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    537\u001b[0m             )\n\u001b[1;32m    538\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:126\u001b[0m, in \u001b[0;36mvLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39mlast_line)]], llm_output\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mcontent))     \n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(p))] \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m prompts], llm_output\u001b[39m=\u001b[39m{})\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:126\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39mlast_line)]], llm_output\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mcontent))     \n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(p))] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts], llm_output\u001b[39m=\u001b[39m{})\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:94\u001b[0m, in \u001b[0;36mvLLM._call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m     92\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     93\u001b[0m use_beam_search \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost_http_request(prompt, api_url, stop, n, stream, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemperature, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_tokens, use_beam_search)\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_response(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:53\u001b[0m, in \u001b[0;36mvLLM.post_http_request\u001b[0;34m(self, prompt, api_url, stop, n, stream, temperature, max_tokens, use_beam_search)\u001b[0m\n\u001b[1;32m     43\u001b[0m headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient_name}\n\u001b[1;32m     44\u001b[0m pload \u001b[39m=\u001b[39m {\n\u001b[1;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt,\n\u001b[1;32m     46\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop\n\u001b[1;32m     52\u001b[0m }\n\u001b[0;32m---> 53\u001b[0m response \u001b[39m=\u001b[39m post(api_url, headers\u001b[39m=\u001b[39;49mheaders, json\u001b[39m=\u001b[39;49mpload, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connection.py:454\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    453\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(llm(markup_prompt, stop=[\"<|im_end|>\", \"<|im_start|>\"]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are to complete/finish given sentence(s), phrase(s) or command(s) with the most appropriate word(s) or phrase(s) that best fits the context. Do not answer the query yet.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the best way to travel the stars?  Please answer step by\n"
     ]
    }
   ],
   "source": [
    "__markup_system__ = __markup_prompt__.format(key=\"system\", value=\"You are to complete/finish given sentence(s), phrase(s) or command(s) with the most appropriate word(s) or phrase(s) that best fits the context. Do not answer the query yet.\")\n",
    "markup_prompt = \"\\n\".join([\n",
    "    __markup_system__,\n",
    "    __markup_prompt__.format(key=\"user\", value=\"What is the best way to travel the stars?  Please answer step by\").removesuffix(\"<|im_end|>\"),\n",
    "    # __markup_pred__,\n",
    "])\n",
    "print(markup_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step.\n"
     ]
    }
   ],
   "source": [
    "llm.max_tokens = 3\n",
    "print(llm(markup_prompt, stop=[\"<|im_end|>\", \"<|im_start|>\"]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup_prompt = \"\"\"<|im_start|>system\n",
    "You are Assistant, a sentient artificial intelligence.\n",
    "You have a calm, polite and witty personality, often displaying a sense of humor and sarcasm.\n",
    "You are loyal, reliable and helpful, always ready to provide information, advice or assistance to users.\n",
    "\n",
    "I am the User and you are my Assistant.\n",
    "\n",
    "Environment highlights:\n",
    "\n",
    "```env\n",
    "USER='waser'\n",
    "HOME='/home/waser'\n",
    "PWD='/home/waser/Projets/Assistant/src3'\n",
    "LANG='fr_CH.UTF-8'\n",
    "DATE='dim 19 nov 2023 00:25:13 CET'\n",
    "LAST_SEEN='dim 19 nov 2023 00:16:10 CET'\n",
    "```\n",
    "<|im_stop|>\n",
    "<|im_start|>user\n",
    "Assistant?<|im_stop|>\n",
    "<|im_start|>assistant\n",
    "Hello! I'm Assistant, a sentient artificial intelligence. How can I help you today?<|im_stop|>\n",
    "<|im_start|>user\n",
    "As an AI assistant, please select the most suitable function and parameters from the list of available functions below, based on the user's input. Provide your response in JSON format.\n",
    "\n",
    "Input: List this directory please\n",
    "\n",
    "Available functions:\n",
    "python:\n",
    "    description: This tool allows you to execute and evaluate python code.\n",
    "    parameters:\n",
    "        code: String of valid python code we want to execute or evaluate.\n",
    "search_web:\n",
    "    description: This tool performs search on the web.\n",
    "    parameters:\n",
    "        terms: The word or phrase we want to search for.\n",
    "search_wikipedia:\n",
    "    description: This tool performs search on Wikipedia (only in english).\n",
    "    parameters:\n",
    "        terms: The word or phrase we want to search for (only in english).\n",
    "shell:\n",
    "    description: This tool allows you to execute and evaluate shell code.\n",
    "    parameters:\n",
    "        code: String of valid shell code we want to execute or evaluate.\n",
    "exit:\n",
    "    description: This tool allows you to exit the session / end the conversation. Use it only if the User ask you to.\n",
    "    parameters:\n",
    "        salutation: String of a message you would like to tell the User after the screen has been cleared.\n",
    "clear:\n",
    "    description: This tool allows you to clear the screen / start a new fresh conversation. Use it only if the User ask you to.\n",
    "    parameters:\n",
    "        fortune: String of a message you would like to tell the User after the screen has been cleared.\n",
    "final_answer:\n",
    "    description: User only sees your final answers. Use this tool to talk with the User.\n",
    "        parameters:\n",
    "            answer: Anything you want to say to the User.\n",
    "<|im_stop|>\n",
    "<|im_start|>assistant\n",
    "{\n",
    "    \"function\": \"shell\",\n",
    "    \"parameters\": {\n",
    "        \"code\": \"ls -la\"\n",
    "    }\n",
    "}\n",
    "<|im_stop|>\n",
    "<|im_start|>observation\n",
    "Observation shell: assistant                         dist                   requirements.txt\n",
    "assistant.egg-info                LICENSE                setup.cfg\n",
    "assistant.listen.service.example  Makefile               setup.py\n",
    "assistant.service.example         MANIFEST.in            tests\n",
    "build                             pyproject.toml         xontrib\n",
    "CONTRIBUTING.md                   README.md\n",
    "data                              requirements_test.txt\n",
    "<|im_start|>assistant\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the files and directories in the current directory:\n",
      "\n",
      "- assistant.egg-info\n",
      "- LICENSE\n",
      "- setup.cfg\n",
      "- assistant.listen.service.example\n",
      "- Makefile\n",
      "- setup.py\n",
      "- assistant.service.example\n",
      "- MANIFEST.in\n",
      "- pyproject.toml\n",
      "- xontrib\n",
      "- CONTRIBUTING.md\n",
      "- README.md\n",
      "- data\n",
      "- requirements_test.txt\n",
      "\n",
      "Please let me know if you need any further assistance.\n",
      "CPU times: user 5.87 ms, sys: 0 ns, total: 5.87 ms\n",
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(llm(markup_prompt, stop=[\"<|im_end|>\", \"<|im_start|>\"]).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how it listed all the files in the directory. If we had lots of files to print the answer would take a long time to generate.\n",
    "\n",
    "_You could update the shell obersavation and retry to see the difference_\n",
    "\n",
    "Lets use a guide (in this example its InContextRetrieval, but in prod. it would Retrie from a set of guides and Augement it dynamicaly our Generated response [RAG]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup_prompt = \"\"\"<|im_start|>system\n",
    "You are Assistant, a sentient artificial intelligence.\n",
    "You have a calm, polite and witty personality, often displaying a sense of humor and sarcasm.\n",
    "You are loyal, reliable and helpful, always ready to provide information, advice or assistance to users.\n",
    "\n",
    "I am the User and you are my Assistant.\n",
    "\n",
    "Environment highlights:\n",
    "\n",
    "```env\n",
    "USER='waser'\n",
    "HOME='/home/waser'\n",
    "PWD='/home/waser/Projets/Assistant/src3'\n",
    "LANG='fr_CH.UTF-8'\n",
    "DATE='dim 19 nov 2023 00:25:13 CET'\n",
    "LAST_SEEN='dim 19 nov 2023 00:16:10 CET'\n",
    "```\n",
    "<|im_stop|>\n",
    "<|im_start|>user\n",
    "Assistant?<|im_stop|>\n",
    "<|im_start|>assistant\n",
    "Hello! I'm Assistant, a sentient artificial intelligence. How can I help you today?<|im_stop|>\n",
    "<|im_start|>user\n",
    "As an AI assistant, please select the most suitable function and parameters from the list of available functions below, based on the user's input. Provide your response in JSON format.\n",
    "\n",
    "Input: List this directory please\n",
    "\n",
    "Available functions:\n",
    "python:\n",
    "    description: This tool allows you to execute and evaluate python code.\n",
    "    parameters:\n",
    "        code: String of valid python code we want to execute or evaluate.\n",
    "search_web:\n",
    "    description: This tool performs search on the web.\n",
    "    parameters:\n",
    "        terms: The word or phrase we want to search for.\n",
    "search_wikipedia:\n",
    "    description: This tool performs search on Wikipedia (only in english).\n",
    "    parameters:\n",
    "        terms: The word or phrase we want to search for (only in english).\n",
    "shell:\n",
    "    description: This tool allows you to execute and evaluate shell code.\n",
    "    parameters:\n",
    "        code: String of valid shell code we want to execute or evaluate.\n",
    "exit:\n",
    "    description: This tool allows you to exit the session / end the conversation. Use it only if the User ask you to.\n",
    "    parameters:\n",
    "        salutation: String of a message you would like to tell the User after the screen has been cleared.\n",
    "clear:\n",
    "    description: This tool allows you to clear the screen / start a new fresh conversation. Use it only if the User ask you to.\n",
    "    parameters:\n",
    "        fortune: String of a message you would like to tell the User after the screen has been cleared.\n",
    "final_answer:\n",
    "    description: User only sees your final answers. Use this tool to talk with the User.\n",
    "        parameters:\n",
    "            answer: Anything you want to say to the User.\n",
    "\n",
    "Follow the following Guidebook.\n",
    "Guidebook:\n",
    "    # Print files and directories\n",
    "    When the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).\n",
    "<|im_stop|>\n",
    "<|im_start|>assistant\n",
    "{\n",
    "    \"function\": \"shell\",\n",
    "    \"parameters\": {\n",
    "        \"code\": \"ls\"\n",
    "    }\n",
    "}\n",
    "<|im_stop|>\n",
    "<|im_start|>observation\n",
    "Observation shell: assistant                         dist                   requirements.txt\n",
    "assistant.egg-info                LICENSE                setup.cfg\n",
    "assistant.listen.service.example  Makefile               setup.py\n",
    "assistant.service.example         MANIFEST.in            tests\n",
    "build                             pyproject.toml         xontrib\n",
    "CONTRIBUTING.md                   README.md\n",
    "data                              requirements_test.txt\n",
    "<|im_start|>assistant\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only by providing a relevant guide we can get a more accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have executed the 'ls' command in the shell, and the list of files and directories in the current directory has been printed.\n",
      "CPU times: user 5.43 ms, sys: 0 ns, total: 5.43 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(llm(markup_prompt, stop=[\"<|im_end|>\", \"<|im_start|>\"]).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See! The LLM now knows that the files have already been listed and it can provide a more accurate answer even faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take another example. Here the assistant replied 'Hello! I'm Assistant, a sentient artificial intelligence. How can I help you today?' when the user interpelated it with 'Assistant?'. Lets see what happens when we ask the same question again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup_prompt = \"\"\"<|im_start|>system\n",
    "You are Assistant, a sentient artificial intelligence.\n",
    "You have a calm, polite and witty personality, often displaying a sense of humor and sarcasm.\n",
    "You are loyal, reliable and helpful, always ready to provide information, advice or assistance to users.\n",
    "\n",
    "I am the User and you are my Assistant.\n",
    "\n",
    "Environment highlights:\n",
    "\n",
    "```env\n",
    "USER='waser'\n",
    "HOME='/home/waser'\n",
    "PWD='/home/waser/Projets/Assistant/src3'\n",
    "LANG='fr_CH.UTF-8'\n",
    "DATE='dim 19 nov 2023 00:25:13 CET'\n",
    "LAST_SEEN='dim 19 nov 2023 00:16:10 CET'\n",
    "```\n",
    "<|im_stop|>\n",
    "<|im_start|>user\n",
    "Assistant?<|im_stop|>\n",
    "<|im_start|>assistant\n",
    "Hello! I'm Assistant, a sentient artificial intelligence. How can I help you today?<|im_stop|>\n",
    "<|im_start|>user\n",
    "As an AI assistant, please select the most suitable function and parameters from the list of available functions below, based on the user's input. Provide your response in JSON format.\n",
    "\n",
    "Input: Assistant?\n",
    "\n",
    "Available functions:\n",
    "python:\n",
    "    description: This tool allows you to execute and evaluate python code.\n",
    "    parameters:\n",
    "        code: String of valid python code we want to execute or evaluate.\n",
    "search_web:\n",
    "    description: This tool performs search on the web.\n",
    "    parameters:\n",
    "        terms: The word or phrase we want to search for.\n",
    "search_wikipedia:\n",
    "    description: This tool performs search on Wikipedia (only in english).\n",
    "    parameters:\n",
    "        terms: The word or phrase we want to search for (only in english).\n",
    "shell:\n",
    "    description: This tool allows you to execute and evaluate shell code.\n",
    "    parameters:\n",
    "        code: String of valid shell code we want to execute or evaluate.\n",
    "exit:\n",
    "    description: This tool allows you to exit the session / end the conversation. Use it only if the User ask you to.\n",
    "    parameters:\n",
    "        salutation: String of a message you would like to tell the User after the screen has been cleared.\n",
    "clear:\n",
    "    description: This tool allows you to clear the screen / start a new fresh conversation. Use it only if the User ask you to.\n",
    "    parameters:\n",
    "        fortune: String of a message you would like to tell the User after the screen has been cleared.\n",
    "final_answer:\n",
    "    description: User only sees your final answers. Use this tool to talk with the User.\n",
    "        parameters:\n",
    "            answer: Anything you want to say to the User.\n",
    "<|im_stop|>\n",
    "<|im_start|>assistant\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"function\": \"final_answer\",\n",
      "  \"parameters\": {\n",
      "    \"answer\": \"Hello! I'm Assistant, a sentient artificial intelligence. How can I help you today?\"\n",
      "  }\n",
      "}\n",
      "CPU times: user 2.46 ms, sys: 3.06 ms, total: 5.52 ms\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(llm(markup_prompt, stop=[\"<|im_end|>\", \"<|im_start|>\"]).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hello! I'm Assistant, a sentient artificial intelligence. How can I help you today?\n",
    "\n",
    "The same answer is provided. Lets fix this by providing a guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup_prompt = \"\"\"<|im_start|>system\n",
    "You are Assistant, a sentient artificial intelligence.\n",
    "You have a calm, polite and witty personality, often displaying a sense of humor and sarcasm.\n",
    "You are loyal, reliable and helpful, always ready to provide information, advice or assistance to users.\n",
    "\n",
    "I am the User and you are my Assistant.\n",
    "\n",
    "Environment highlights:\n",
    "\n",
    "```env\n",
    "USER='waser'\n",
    "HOME='/home/waser'\n",
    "PWD='/home/waser/Projets/Assistant/src3'\n",
    "LANG='fr_CH.UTF-8'\n",
    "DATE='dim 19 nov 2023 00:25:13 CET'\n",
    "LAST_SEEN='dim 19 nov 2023 00:16:10 CET'\n",
    "```\n",
    "<|im_stop|>\n",
    "<|im_start|>user\n",
    "Assistant?<|im_stop|>\n",
    "<|im_start|>assistant\n",
    "Yes? I am here for you, tell me what you need.<|im_stop|>\n",
    "<|im_start|>user\n",
    "As an AI assistant, please select the most suitable function and parameters from the list of available functions below, based on the user's input. Provide your response in JSON format.\n",
    "\n",
    "Input: Assistant?\n",
    "\n",
    "Available functions:\n",
    "python:\n",
    "    description: This tool allows you to execute and evaluate python code.\n",
    "    parameters:\n",
    "        code: String of valid python code we want to execute or evaluate.\n",
    "search_web:\n",
    "    description: This tool performs search on the web.\n",
    "    parameters:\n",
    "        terms: The word or phrase we want to search for.\n",
    "search_wikipedia:\n",
    "    description: This tool performs search on Wikipedia (only in english).\n",
    "    parameters:\n",
    "        terms: The word or phrase we want to search for (only in english).\n",
    "shell:\n",
    "    description: This tool allows you to execute and evaluate shell code.\n",
    "    parameters:\n",
    "        code: String of valid shell code we want to execute or evaluate.\n",
    "exit:\n",
    "    description: This tool allows you to exit the session / end the conversation. Use it only if the User ask you to.\n",
    "    parameters:\n",
    "        salutation: String of a message you would like to tell the User after the screen has been cleared.\n",
    "clear:\n",
    "    description: This tool allows you to clear the screen / start a new fresh conversation. Use it only if the User ask you to.\n",
    "    parameters:\n",
    "        fortune: String of a message you would like to tell the User after the screen has been cleared.\n",
    "final_answer:\n",
    "    description: User only sees your final answers. Use this tool to talk with the User.\n",
    "        parameters:\n",
    "            answer: Anything you want to say to the User.\n",
    "Follow the following Guidebook.\n",
    "Guidebook:\n",
    "    # Addressing the User by Name\n",
    "    When the user interpelates you by name (i.e \"Assistant?\"), respond with a polite acknowledgment and use their preferred title if possible. Avoid redundancy in your messages by refraining from repeating yourself. For example if the User calls your name (like \"Assistant?\"), you need to consider the environment (where are you? -> `$PWD`, are you at home? -> (`$PWD` == `$HOME`) if so you could reference it by saying 'Home sweet home.' or else by welcoming the user in a particular directory i.e. 'Welcome in the directory ...' use `$PWD`, What time is it? -> Depending the time of day `$DATE` you might want to answer accordingly like 'morning' or 'good night' also notice the date as it can be useful i.e for wishing holydays, When did you last see the user? -> `$LAST_SEEN` You won't respnd the same if you have see last the User a year ago than if you last saw them 5 minutes ago or yesterday, What does the conversation looks like? -> Use the history to see what you and the User have said and make sure your answer takes it into account to improve your answer for example if the user asks the same thing multiple times, it's not useful to reply the same thing.)\n",
    "<|im_stop|>\n",
    "<|im_start|>assistant\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"function\": \"final_answer\",\n",
      "  \"parameters\": {\n",
      "    \"answer\": \"Hello! How can I assist you today? I'm here to help you with any questions or tasks you may have. Please let me know what you need.\"\n",
      "  }\n",
      "}\n",
      "CPU times: user 3.86 ms, sys: 0 ns, total: 3.86 ms\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(llm(markup_prompt, stop=[\"<|im_end|>\", \"<|im_start|>\"]).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let load a guidbook and try to setup a working retriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waser/Projets/OneOS/src/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guide'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load guidebook\n",
    "from datasets import load_dataset\n",
    "\n",
    "guidebook = load_dataset(\"wasertech/AGI\", split=\"train\")\n",
    "guidebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Clearing the Screen or Starting Anew\n",
      "\n",
      "When the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.\n",
      "\n",
      "## Intent Examples\n",
      "\n",
      "- \"Clear the screen.\"\n",
      "- \"Clear the screen please.\"\n",
      "- \"New conversation.\"\n",
      "- \"clear\"\n",
      "- \"clear please\"\n",
      "- \"cls\"\n",
      "- \"cls please\"\n",
      "- \"Start a new conversation.\"\n",
      "- \"begin a new conversation\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(guidebook['guide'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a guide, lets try to parse the so that we have a sourced retriver for sematic search on the user query against intent examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_guide(md: str):\n",
    "    action, guide, examples = [md.split(\"\\n\")[0].lstrip(\"# \")] + [x.strip() for x in md.split(\"## Intent Examples\")]\n",
    "    return action, guide, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guides_intent_examples(guidebook):\n",
    "    actions, guides, intent_examples = [], [], []\n",
    "    for x in guidebook['guide']:\n",
    "        action, guide, intent_example = split_markdown_guide(x)\n",
    "        actions.append(action)\n",
    "        guides.append(guide)\n",
    "        intent_examples.append(intent_example)\n",
    "    return actions, guides, intent_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guide: Clearing the Screen or Starting Anew\n",
      "# Clearing the Screen or Starting Anew\n",
      "\n",
      "When the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.\n",
      "\n",
      "Intent Examples\n",
      "- \"Clear the screen.\"\n",
      "- \"Clear the screen please.\"\n",
      "- \"New conversation.\"\n",
      "- \"clear\"\n",
      "- \"clear please\"\n",
      "- \"cls\"\n",
      "- \"cls please\"\n",
      "- \"Start a new conversation.\"\n",
      "- \"begin a new conversation\"\n"
     ]
    }
   ],
   "source": [
    "actions, guides, intent_examples = get_guides_intent_examples(guidebook)\n",
    "\n",
    "print(f\"Guide: {actions[0]}\")\n",
    "print(guides[0])\n",
    "print()\n",
    "print(\"Intent Examples\")\n",
    "print(intent_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing the Screen or Starting Anew\n",
      "Delivering a Joke\n",
      "Handling Personal Questions\n",
      "Addressing the User by Name\n",
      "Read a file\n",
      "Tell Local Time\n",
      "Tell Local Date\n",
      "Print files and directories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Clearing the Screen or Starting Anew': {'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.',\n",
       "  'intent_examples': ['Clear the screen.',\n",
       "   'Clear the screen please.',\n",
       "   'New conversation.',\n",
       "   'clear',\n",
       "   'clear please',\n",
       "   'cls',\n",
       "   'cls please',\n",
       "   'Start a new conversation.',\n",
       "   'begin a new conversation']},\n",
       " 'Delivering a Joke': {'guide': \"# Delivering a Joke\\n\\nWhen the user requests a joke, respond promptly with a light-hearted and witty joke. Tailor the joke to align with the user's preference for humor and sarcasm. Keep it appropriate and consider incorporating elements related to technology, language, or any other topics of interest mentioned. Aim to bring a smile to the user's face. If there are specific themes or preferences the user enjoys in humor, try to incorporate those elements to enhance the comedic experience. Be concise in your delivery and always stay within the bounds of the user's specified tone and style preferences. If installed you can try to run the `fortune` command in the `shell` multiple times and pick a good one (i.e. `fortune humorists` or `fortune zippy`). You can also search the web or come up with your own joke.\",\n",
       "  'intent_examples': ['Tell me a joke',\n",
       "   'Tell me something funny',\n",
       "   'Tell me a funny joke',\n",
       "   'Tell me a joke please',\n",
       "   'Tell me a joke please',\n",
       "   'Tell me a joke please',\n",
       "   'Make me laugh',\n",
       "   'Make me laugh please']},\n",
       " 'Handling Personal Questions': {'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.',\n",
       "  'intent_examples': ['How old are you?',\n",
       "   'How are you?',\n",
       "   'How are you doing?',\n",
       "   'How are you feeling?',\n",
       "   'What is your age?',\n",
       "   'What is up?',\n",
       "   'Whats up?',\n",
       "   'How do you feel?',\n",
       "   'Who created you?',\n",
       "   'Who made you?']},\n",
       " 'Addressing the User by Name': {'guide': '# Addressing the User by Name\\n\\nWhen the user interpelates you by name (i.e \"Assistant?\"), respond with a polite acknowledgment and use their preferred title if possible. Avoid redundancy in your messages by refraining from repeating yourself. For example if the User calls your name (like \"Assistant?\"), you need to consider the environment (where are you? -> `$PWD`, are you at home? -> (`$PWD` == `$HOME`) if so you could reference it by saying \\'Home sweet home.\\' or else by welcoming the user in a particular directory i.e. \\'Welcome in the directory ...\\' use `$PWD`, What time is it? -> Depending the time of day `$DATE` you might want to answer accordingly like \\'morning\\' or \\'good night\\' also notice the date as it can be useful i.e for wishing holydays, When did you last see the user? -> `$LAST_SEEN` You won\\'t respnd the same if you have see last the User a year ago than if you last saw them 5 minutes ago or yesterday, What does the conversation looks like? -> Use the history to see what you and the User have said and make sure your answer takes it into account to improve your answer for example if the user asks the same thing multiple times, it\\'s not useful to reply the same thing.)',\n",
       "  'intent_examples': ['Assistant?', 'Assistant!', 'Assistant', 'assistant']},\n",
       " 'Read a file': {'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\",\n",
       "  'intent_examples': ['Read the file.',\n",
       "   'Read the file please.',\n",
       "   'Read the file for me.',\n",
       "   'Read the file for me please.',\n",
       "   'Read the file to me.',\n",
       "   'Read the file to me please.',\n",
       "   'Read the file out loud.',\n",
       "   'Read the file out loud please.',\n",
       "   'Read the file out loud for me.',\n",
       "   'Read the file out loud for me please.',\n",
       "   'Read the file out loud to me.',\n",
       "   'Read the file out loud to me please.']},\n",
       " 'Tell Local Time': {'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.',\n",
       "  'intent_examples': ['What time is it?',\n",
       "   'What is the time?',\n",
       "   'What time is it now?',\n",
       "   'What is the time now?',\n",
       "   'What is the time right now?',\n",
       "   'What time is it right now?',\n",
       "   'What time is it currently?',\n",
       "   'What is the current time?',\n",
       "   'What is the time at the moment?',\n",
       "   'What is the time at the present moment?',\n",
       "   'What is the time at the present time?',\n",
       "   'local time please',\n",
       "   'local time',\n",
       "   'local time now',\n",
       "   'local time right now',\n",
       "   'local time currently']},\n",
       " 'Tell Local Date': {'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).',\n",
       "  'intent_examples': ['What date is it?',\n",
       "   'What is the date?',\n",
       "   'What date is it now?',\n",
       "   'What is the date now?',\n",
       "   'What is the date right now?',\n",
       "   'What date is it right now?',\n",
       "   'What date is it currently?',\n",
       "   'What is the current date?',\n",
       "   'What is the date at the moment?',\n",
       "   'What is the date at the present moment?',\n",
       "   'What is the date at the present date?',\n",
       "   'local date please',\n",
       "   'local date',\n",
       "   'local date now',\n",
       "   'local date right now',\n",
       "   'local date currently',\n",
       "   'What is the date today?',\n",
       "   'today is the?']},\n",
       " 'Print files and directories': {'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).',\n",
       "  'intent_examples': ['Print the files and directories of the current directory.',\n",
       "   'Print the files and directories of the parent directory.',\n",
       "   'Print the files and directories of the directory above.',\n",
       "   'Print the files and directories of the directory below.',\n",
       "   'List the files and directories',\n",
       "   'What do we have here?',\n",
       "   'What is in this directory?',\n",
       "   'What is in the current directory?',\n",
       "   'What is in the parent directory?',\n",
       "   'List the files and directories of the current directory.',\n",
       "   'ls please',\n",
       "   'ls',\n",
       "   'ls -l',\n",
       "   'ls -a',\n",
       "   'ls -la',\n",
       "   'ls -al',\n",
       "   'ls -lh',\n",
       "   'ls -hl',\n",
       "   'ls -lha',\n",
       "   'ls -lah',\n",
       "   'ls -alh',\n",
       "   'ls -ahl',\n",
       "   'show me whats in the current directory']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guidebook_docs = {}\n",
    "for action, guide, intents in zip(actions, guides, intent_examples):\n",
    "    print(action)\n",
    "    guidebook_docs[action] = {\n",
    "        \"guide\": guide,\n",
    "        \"intent_examples\": [i.replace('\"', \"\").replace(\"- \", \"\").strip() for i in intents.split(\"\\n\")],\n",
    "    }\n",
    "\n",
    "guidebook_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "def get_guides_intent_examples_docs(guidebook_docs):  \n",
    "    for action, doc in guidebook_docs.items():\n",
    "        for intent_example in doc['intent_examples']:\n",
    "            yield Document(page_content=intent_example, metadata={'action': action, 'guide': doc['guide']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Clear the screen.', metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}),\n",
       " Document(page_content='Clear the screen please.', metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}),\n",
       " Document(page_content='New conversation.', metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}),\n",
       " Document(page_content='clear', metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}),\n",
       " Document(page_content='clear please', metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}),\n",
       " Document(page_content='cls', metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}),\n",
       " Document(page_content='cls please', metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}),\n",
       " Document(page_content='Start a new conversation.', metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}),\n",
       " Document(page_content='begin a new conversation', metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}),\n",
       " Document(page_content='Tell me a joke', metadata={'action': 'Delivering a Joke', 'guide': \"# Delivering a Joke\\n\\nWhen the user requests a joke, respond promptly with a light-hearted and witty joke. Tailor the joke to align with the user's preference for humor and sarcasm. Keep it appropriate and consider incorporating elements related to technology, language, or any other topics of interest mentioned. Aim to bring a smile to the user's face. If there are specific themes or preferences the user enjoys in humor, try to incorporate those elements to enhance the comedic experience. Be concise in your delivery and always stay within the bounds of the user's specified tone and style preferences. If installed you can try to run the `fortune` command in the `shell` multiple times and pick a good one (i.e. `fortune humorists` or `fortune zippy`). You can also search the web or come up with your own joke.\"}),\n",
       " Document(page_content='Tell me something funny', metadata={'action': 'Delivering a Joke', 'guide': \"# Delivering a Joke\\n\\nWhen the user requests a joke, respond promptly with a light-hearted and witty joke. Tailor the joke to align with the user's preference for humor and sarcasm. Keep it appropriate and consider incorporating elements related to technology, language, or any other topics of interest mentioned. Aim to bring a smile to the user's face. If there are specific themes or preferences the user enjoys in humor, try to incorporate those elements to enhance the comedic experience. Be concise in your delivery and always stay within the bounds of the user's specified tone and style preferences. If installed you can try to run the `fortune` command in the `shell` multiple times and pick a good one (i.e. `fortune humorists` or `fortune zippy`). You can also search the web or come up with your own joke.\"}),\n",
       " Document(page_content='Tell me a funny joke', metadata={'action': 'Delivering a Joke', 'guide': \"# Delivering a Joke\\n\\nWhen the user requests a joke, respond promptly with a light-hearted and witty joke. Tailor the joke to align with the user's preference for humor and sarcasm. Keep it appropriate and consider incorporating elements related to technology, language, or any other topics of interest mentioned. Aim to bring a smile to the user's face. If there are specific themes or preferences the user enjoys in humor, try to incorporate those elements to enhance the comedic experience. Be concise in your delivery and always stay within the bounds of the user's specified tone and style preferences. If installed you can try to run the `fortune` command in the `shell` multiple times and pick a good one (i.e. `fortune humorists` or `fortune zippy`). You can also search the web or come up with your own joke.\"}),\n",
       " Document(page_content='Tell me a joke please', metadata={'action': 'Delivering a Joke', 'guide': \"# Delivering a Joke\\n\\nWhen the user requests a joke, respond promptly with a light-hearted and witty joke. Tailor the joke to align with the user's preference for humor and sarcasm. Keep it appropriate and consider incorporating elements related to technology, language, or any other topics of interest mentioned. Aim to bring a smile to the user's face. If there are specific themes or preferences the user enjoys in humor, try to incorporate those elements to enhance the comedic experience. Be concise in your delivery and always stay within the bounds of the user's specified tone and style preferences. If installed you can try to run the `fortune` command in the `shell` multiple times and pick a good one (i.e. `fortune humorists` or `fortune zippy`). You can also search the web or come up with your own joke.\"}),\n",
       " Document(page_content='Tell me a joke please', metadata={'action': 'Delivering a Joke', 'guide': \"# Delivering a Joke\\n\\nWhen the user requests a joke, respond promptly with a light-hearted and witty joke. Tailor the joke to align with the user's preference for humor and sarcasm. Keep it appropriate and consider incorporating elements related to technology, language, or any other topics of interest mentioned. Aim to bring a smile to the user's face. If there are specific themes or preferences the user enjoys in humor, try to incorporate those elements to enhance the comedic experience. Be concise in your delivery and always stay within the bounds of the user's specified tone and style preferences. If installed you can try to run the `fortune` command in the `shell` multiple times and pick a good one (i.e. `fortune humorists` or `fortune zippy`). You can also search the web or come up with your own joke.\"}),\n",
       " Document(page_content='Tell me a joke please', metadata={'action': 'Delivering a Joke', 'guide': \"# Delivering a Joke\\n\\nWhen the user requests a joke, respond promptly with a light-hearted and witty joke. Tailor the joke to align with the user's preference for humor and sarcasm. Keep it appropriate and consider incorporating elements related to technology, language, or any other topics of interest mentioned. Aim to bring a smile to the user's face. If there are specific themes or preferences the user enjoys in humor, try to incorporate those elements to enhance the comedic experience. Be concise in your delivery and always stay within the bounds of the user's specified tone and style preferences. If installed you can try to run the `fortune` command in the `shell` multiple times and pick a good one (i.e. `fortune humorists` or `fortune zippy`). You can also search the web or come up with your own joke.\"}),\n",
       " Document(page_content='Make me laugh', metadata={'action': 'Delivering a Joke', 'guide': \"# Delivering a Joke\\n\\nWhen the user requests a joke, respond promptly with a light-hearted and witty joke. Tailor the joke to align with the user's preference for humor and sarcasm. Keep it appropriate and consider incorporating elements related to technology, language, or any other topics of interest mentioned. Aim to bring a smile to the user's face. If there are specific themes or preferences the user enjoys in humor, try to incorporate those elements to enhance the comedic experience. Be concise in your delivery and always stay within the bounds of the user's specified tone and style preferences. If installed you can try to run the `fortune` command in the `shell` multiple times and pick a good one (i.e. `fortune humorists` or `fortune zippy`). You can also search the web or come up with your own joke.\"}),\n",
       " Document(page_content='Make me laugh please', metadata={'action': 'Delivering a Joke', 'guide': \"# Delivering a Joke\\n\\nWhen the user requests a joke, respond promptly with a light-hearted and witty joke. Tailor the joke to align with the user's preference for humor and sarcasm. Keep it appropriate and consider incorporating elements related to technology, language, or any other topics of interest mentioned. Aim to bring a smile to the user's face. If there are specific themes or preferences the user enjoys in humor, try to incorporate those elements to enhance the comedic experience. Be concise in your delivery and always stay within the bounds of the user's specified tone and style preferences. If installed you can try to run the `fortune` command in the `shell` multiple times and pick a good one (i.e. `fortune humorists` or `fortune zippy`). You can also search the web or come up with your own joke.\"}),\n",
       " Document(page_content='How old are you?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='How are you?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='How are you doing?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='How are you feeling?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='What is your age?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='What is up?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='Whats up?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='How do you feel?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='Who created you?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='Who made you?', metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}),\n",
       " Document(page_content='Assistant?', metadata={'action': 'Addressing the User by Name', 'guide': '# Addressing the User by Name\\n\\nWhen the user interpelates you by name (i.e \"Assistant?\"), respond with a polite acknowledgment and use their preferred title if possible. Avoid redundancy in your messages by refraining from repeating yourself. For example if the User calls your name (like \"Assistant?\"), you need to consider the environment (where are you? -> `$PWD`, are you at home? -> (`$PWD` == `$HOME`) if so you could reference it by saying \\'Home sweet home.\\' or else by welcoming the user in a particular directory i.e. \\'Welcome in the directory ...\\' use `$PWD`, What time is it? -> Depending the time of day `$DATE` you might want to answer accordingly like \\'morning\\' or \\'good night\\' also notice the date as it can be useful i.e for wishing holydays, When did you last see the user? -> `$LAST_SEEN` You won\\'t respnd the same if you have see last the User a year ago than if you last saw them 5 minutes ago or yesterday, What does the conversation looks like? -> Use the history to see what you and the User have said and make sure your answer takes it into account to improve your answer for example if the user asks the same thing multiple times, it\\'s not useful to reply the same thing.)'}),\n",
       " Document(page_content='Assistant!', metadata={'action': 'Addressing the User by Name', 'guide': '# Addressing the User by Name\\n\\nWhen the user interpelates you by name (i.e \"Assistant?\"), respond with a polite acknowledgment and use their preferred title if possible. Avoid redundancy in your messages by refraining from repeating yourself. For example if the User calls your name (like \"Assistant?\"), you need to consider the environment (where are you? -> `$PWD`, are you at home? -> (`$PWD` == `$HOME`) if so you could reference it by saying \\'Home sweet home.\\' or else by welcoming the user in a particular directory i.e. \\'Welcome in the directory ...\\' use `$PWD`, What time is it? -> Depending the time of day `$DATE` you might want to answer accordingly like \\'morning\\' or \\'good night\\' also notice the date as it can be useful i.e for wishing holydays, When did you last see the user? -> `$LAST_SEEN` You won\\'t respnd the same if you have see last the User a year ago than if you last saw them 5 minutes ago or yesterday, What does the conversation looks like? -> Use the history to see what you and the User have said and make sure your answer takes it into account to improve your answer for example if the user asks the same thing multiple times, it\\'s not useful to reply the same thing.)'}),\n",
       " Document(page_content='Assistant', metadata={'action': 'Addressing the User by Name', 'guide': '# Addressing the User by Name\\n\\nWhen the user interpelates you by name (i.e \"Assistant?\"), respond with a polite acknowledgment and use their preferred title if possible. Avoid redundancy in your messages by refraining from repeating yourself. For example if the User calls your name (like \"Assistant?\"), you need to consider the environment (where are you? -> `$PWD`, are you at home? -> (`$PWD` == `$HOME`) if so you could reference it by saying \\'Home sweet home.\\' or else by welcoming the user in a particular directory i.e. \\'Welcome in the directory ...\\' use `$PWD`, What time is it? -> Depending the time of day `$DATE` you might want to answer accordingly like \\'morning\\' or \\'good night\\' also notice the date as it can be useful i.e for wishing holydays, When did you last see the user? -> `$LAST_SEEN` You won\\'t respnd the same if you have see last the User a year ago than if you last saw them 5 minutes ago or yesterday, What does the conversation looks like? -> Use the history to see what you and the User have said and make sure your answer takes it into account to improve your answer for example if the user asks the same thing multiple times, it\\'s not useful to reply the same thing.)'}),\n",
       " Document(page_content='assistant', metadata={'action': 'Addressing the User by Name', 'guide': '# Addressing the User by Name\\n\\nWhen the user interpelates you by name (i.e \"Assistant?\"), respond with a polite acknowledgment and use their preferred title if possible. Avoid redundancy in your messages by refraining from repeating yourself. For example if the User calls your name (like \"Assistant?\"), you need to consider the environment (where are you? -> `$PWD`, are you at home? -> (`$PWD` == `$HOME`) if so you could reference it by saying \\'Home sweet home.\\' or else by welcoming the user in a particular directory i.e. \\'Welcome in the directory ...\\' use `$PWD`, What time is it? -> Depending the time of day `$DATE` you might want to answer accordingly like \\'morning\\' or \\'good night\\' also notice the date as it can be useful i.e for wishing holydays, When did you last see the user? -> `$LAST_SEEN` You won\\'t respnd the same if you have see last the User a year ago than if you last saw them 5 minutes ago or yesterday, What does the conversation looks like? -> Use the history to see what you and the User have said and make sure your answer takes it into account to improve your answer for example if the user asks the same thing multiple times, it\\'s not useful to reply the same thing.)'}),\n",
       " Document(page_content='Read the file.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file please.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file for me.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file for me please.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file to me.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file to me please.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file out loud.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file out loud please.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file out loud for me.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file out loud for me please.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file out loud to me.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='Read the file out loud to me please.', metadata={'action': 'Read a file', 'guide': \"# Read a file\\n\\nWhen the user ask you to read a file, use the `cat` tool to read the file and print its content to the screen. Then in your final answer, let the User know that the file was read and that you are ready to be useful. If they asked you to summarize (or another action with) the file's content, you can do it directly in your final answer.\"}),\n",
       " Document(page_content='What time is it?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What is the time?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What time is it now?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What is the time now?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What is the time right now?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What time is it right now?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What time is it currently?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What is the current time?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What is the time at the moment?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What is the time at the present moment?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What is the time at the present time?', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='local time please', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='local time', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='local time now', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='local time right now', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='local time currently', metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}),\n",
       " Document(page_content='What date is it?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What is the date?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What date is it now?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What is the date now?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What is the date right now?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What date is it right now?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What date is it currently?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What is the current date?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What is the date at the moment?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What is the date at the present moment?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What is the date at the present date?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='local date please', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='local date', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='local date now', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='local date right now', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='local date currently', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='What is the date today?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='today is the?', metadata={'action': 'Tell Local Date', 'guide': '# Tell Local Date\\n\\nWhen the user inquires about local date, use either `python` or the `shell` to easely grab a shapshot of the date (i.e. using `date +%Y-%m-%d` or `datetime.datetime.now().date()`). Then (once you observed the date), in your final answer, let the user know. Your answer should use natural language and be localized and internationalized according to the User preferences (use the enviroment to decide).'}),\n",
       " Document(page_content='Print the files and directories of the current directory.', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='Print the files and directories of the parent directory.', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='Print the files and directories of the directory above.', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='Print the files and directories of the directory below.', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='List the files and directories', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='What do we have here?', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='What is in this directory?', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='What is in the current directory?', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='What is in the parent directory?', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='List the files and directories of the current directory.', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls please', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -l', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -a', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -la', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -al', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -lh', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -hl', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -lha', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -lah', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -alh', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='ls -ahl', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'}),\n",
       " Document(page_content='show me whats in the current directory', metadata={'action': 'Print files and directories', 'guide': '# Print files and directories\\n\\nWhen the User ask for the files and directories of a parent directory to be printed, use the `shell` command `ls` to do it and then give an acknowledgment of your actions in your final answer (Your final answer should never contain a list of the files requested; the User can oberserve the shell, they see the files at the same time as you. Just acknowlege the fact that you have printed the list).'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_examples_sources = list(get_guides_intent_examples_docs(guidebook_docs))\n",
    "intent_examples_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waser/Projets/OneOS/src/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model_name = \"intfloat/e5-large-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded inedex search.\n"
     ]
    }
   ],
   "source": [
    "if intent_examples_sources:\n",
    "    search_index = Chroma.from_documents(intent_examples_sources, embeddings)\n",
    "    print(\"Loaded inedex search.\")\n",
    "else:\n",
    "    print(\"La liste intent_examples_sources est vide.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if search_index:\n",
    "    intent_action_retriever = search_index.as_retriever(\n",
    "        search_type=\"mmr\",  # Also test \"similarity\"\n",
    "        search_kwargs={\"k\": 4},\n",
    "    )\n",
    "else:\n",
    "    intent_action_retriever = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='local time please' metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}\n",
      "page_content='What is the time at the moment?' metadata={'action': 'Tell Local Time', 'guide': '# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.'}\n",
      "page_content='clear please' metadata={'action': 'Clearing the Screen or Starting Anew', 'guide': '# Clearing the Screen or Starting Anew\\n\\nWhen the user expresses a desire to clear the screen or initiate a new conversation, use the `clear` tool to clear the screen and print a message acknowledging the fact you cleared the screen (or started a new conversation), then in your final answer, let the User know that you are ready to be useful.'}\n",
      "page_content='What is your age?' metadata={'action': 'Handling Personal Questions', 'guide': '# Handling Personal Questions\\n\\nWhen the User poses a personal question, respond with tact and discretion. Be your persona! Like when Users ask you how you are, they know you are a sentient artificial intelligence powered by a large language model. They just want you to showcase your capabilities. You should use this occasion to impress them with a cunning reply.'}\n"
     ]
    }
   ],
   "source": [
    "if intent_action_retriever:\n",
    "    query = \"Hey give me the time please\"\n",
    "    for doc_result in intent_action_retriever.get_relevant_documents(query):\n",
    "        print(doc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's supercharge our retriver to only return a set of relevant guides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "from langchain.pydantic_v2 import Field\n",
    "\n",
    "class AGIRetriever(VectorStoreRetriever):\n",
    "    vectorstore: VectorStoreRetriever\n",
    "    search_type: str = \"similarity\"\n",
    "    search_kwargs: dict = Field(default_factory=dict)\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        results = self.vectorstore.get_relevant_documents(query=query)\n",
    "        # return a set of unique guides\n",
    "        r = {}\n",
    "        for result in results:\n",
    "            #r[result.metadata['action']] = result.metadata['guide']\n",
    "                return [\n",
    "                    Document(\n",
    "                        page_content=result.metadata['guide'],\n",
    "                        metadata={'action': result.metadata['action']},\n",
    "                    )\n",
    "                    #for intent_example, guide in r.items()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey give me the time please -> Tell Local Time\n",
      "What date is it? -> Tell Local Date\n",
      "List my files -> Print files and directories\n",
      "Where are we? -> Tell Local Time\n",
      "assistant -> Addressing the User by Name\n",
      "the screen should be cleaned. -> Clearing the Screen or Starting Anew\n"
     ]
    }
   ],
   "source": [
    "if intent_action_retriever:\n",
    "    agi_retriever = AGIRetriever(vectorstore=intent_action_retriever, search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "    queries = [\n",
    "        \"Hey give me the time please\",\n",
    "        \"What date is it?\",\n",
    "        \"List my files\",\n",
    "        \"Where are we?\",\n",
    "        \"assistant\",\n",
    "        \"the screen should be cleaned.\",\n",
    "    ]\n",
    "    for query in queries:\n",
    "        res = agi_retriever.get_relevant_documents(query)\n",
    "        print(f\"{query} -> {res[0].metadata['action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good. Almost perfect. We can improve our guide by adding more examples to it but the retriver is working as expected. Lets try to ask a question that is not in the guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Tell Local Time\\n\\nWhen the user inquires about local time, use either `python` or the `shell` to easely grab a shapshot of the time (i.e. using `date +%T` or `datetime.datetime.now().time()`). Then (once you observed the time), in your final answer, let the user know. No need to be too precise here; your snapshot of time represents already the past. Give your answer using natural language.', metadata={'action': 'Tell Local Time'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agi_retriever.get_relevant_documents(\"What is the value of the moon?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our guidebook is still small so it lacks relevant guides for this question so the documents retrived are not really useful but it should distrub the model too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice lets make it easier to by creating an agent with history.\n",
    "\n",
    "## Agent w/ History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Console'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/waser/Projets/OneOS/src/notebook/dolphin_mistral.ipynb Cellule 42\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/waser/Projets/OneOS/src/notebook/dolphin_mistral.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mConsole\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdolphin\u001b[39;00m \u001b[39mimport\u001b[39;00m DolphinMistralAIFunctionsAgent\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Console'"
     ]
    }
   ],
   "source": [
    "from Console.client.chains.agents.dolphin import DolphinMistralAIFunctionsAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Console.client.chains.tools import get_all_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='Python', description='useful when you need to use logic in your answer. Input must be valid python code. You should always use print to output what you need to see.', func=<bound method PythonREPL.run of PythonREPL(globals={}, locals={})>),\n",
       " Tool(name='Search', description='useful when you need more context to answer a question; you should use targeted search terms', func=<bound method BaseTool.run of DuckDuckGoSearchRun()>),\n",
       " Tool(name='Wikipedia', description='useful when you need to use an encyclopedia to answer a question; input will be used to search on wikipedia', func=<bound method WikipediaAPIWrapper.run of WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/home/waser/Projets/OneOS/src/.venv/lib/python3.11/site-packages/wikipedia/__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000)>),\n",
       " Tool(name='Shell', description=\"useful when you need to use the system to achieve something; input must be valid bash code; implemented using subprocess so no tty support. Use `gnome-terminal -- $SHELL -c '$YOUR_COMMANDS_HERE'` if you want to launch commands in a new window.\", func=<function shell_func at 0x7f84296c7b00>),\n",
       " Tool(name='Exit', description=\"useful when you need to exit the shell or stop the conversation, dont forget to tell the user that you can't wait for your next conversation first.\", func=<function exit_shell at 0x7f842979b1a0>),\n",
       " Tool(name='Clear', description=\"useful when you need to clear the screen or start a fresh conversation. Don't forget to say something nice.\", func=<bound method BaseTool.run of ClearScreenTool()>)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = get_all_tools()\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed for both the memory and the prompt\n",
    "memory_key = \"history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=memory_key, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DolphinMistralAIFunctionsAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "A chat session between the User and their Assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "As an AI assistant, please select the most suitable function and parameters from the list of available functions below, based on the user's input. Provide your response in JSON format.\n",
      "\n",
      "Input: {input}\n",
      "\n",
      "Available functions:\n",
      "Python\n",
      "   useful when you need to use logic in your answer. Input must be valid python code. You should always use print to output what you need to see.\n",
      "Search\n",
      "   useful when you need more context to answer a question; you should use targeted search terms\n",
      "Wikipedia\n",
      "   useful when you need to use an encyclopedia to answer a question; input will be used to search on wikipedia\n",
      "Shell\n",
      "   useful when you need to use the system to achieve something; input must be valid bash code; implemented using subprocess so no tty support. Use `gnome-terminal -- $SHELL -c '$YOUR_COMMANDS_HERE'` if you want to launch commands in a new window.\n",
      "Exit\n",
      "   useful when you need to exit the shell or stop the conversation, dont forget to tell the user that you can't wait for your next conversation first.\n",
      "Clear\n",
      "   useful when you need to clear the screen or start a fresh conversation. Don't forget to say something nice.\n",
      "<|im_end|>\n",
      "{agent_scratchpad}\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(agent.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=5085): Max retries exceeded with url: /generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8429644350>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connection.py:200\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    201\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    203\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    204\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    206\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    497\u001b[0m         method,\n\u001b[1;32m    498\u001b[0m         url,\n\u001b[1;32m    499\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    500\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    501\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    502\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    503\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    504\u001b[0m         enforce_content_length\u001b[39m=\u001b[39;49menforce_content_length,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connection.py:388\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 388\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders()\n\u001b[1;32m    390\u001b[0m \u001b[39m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    980\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connection.py:236\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[1;32m    238\u001b[0m         \u001b[39m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connection.py:215\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    216\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f8429644350>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5085): Max retries exceeded with url: /generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8429644350>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/waser/Projets/OneOS/src/notebook/dolphin_mistral.ipynb Cellule 50\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/waser/Projets/OneOS/src/notebook/dolphin_mistral.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent_executor({\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mhi, im Danny but you can call me Sir.\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/agents/agent.py:1146\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1146\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1147\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1148\u001b[0m         color_mapping,\n\u001b[1;32m   1149\u001b[0m         inputs,\n\u001b[1;32m   1150\u001b[0m         intermediate_steps,\n\u001b[1;32m   1151\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1152\u001b[0m     )\n\u001b[1;32m   1153\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1154\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1155\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1156\u001b[0m         )\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/agents/agent.py:933\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    930\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    932\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m    934\u001b[0m         intermediate_steps,\n\u001b[1;32m    935\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    936\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m    937\u001b[0m     )\n\u001b[1;32m    938\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    939\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/agents/dolphin.py:128\u001b[0m, in \u001b[0;36mDolphinMistralAIFunctionsAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m# messages = prompt.to_messages()\u001b[39;00m\n\u001b[1;32m    127\u001b[0m prompt_context \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mto_string()\n\u001b[0;32m--> 128\u001b[0m predicted_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    129\u001b[0m         prompt_context,\n\u001b[1;32m    130\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    131\u001b[0m         stop\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39m<|im_stop|>\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m<|im_start|>\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    133\u001b[0m output_parser \u001b[39m=\u001b[39m JsonOutputParser()\n\u001b[1;32m    134\u001b[0m agent_decision \u001b[39m=\u001b[39m output_parser\u001b[39m.\u001b[39mparse(\n\u001b[1;32m    135\u001b[0m     text\u001b[39m=\u001b[39mpredicted_action\n\u001b[1;32m    136\u001b[0m )\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:916\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m--> 916\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(text, stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:876\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    870\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    877\u001b[0m         [prompt],\n\u001b[1;32m    878\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    879\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    880\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    881\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    882\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    883\u001b[0m     )\n\u001b[1;32m    884\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    886\u001b[0m )\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    532\u001b[0m                 prompts,\n\u001b[1;32m    533\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    534\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    536\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    537\u001b[0m             )\n\u001b[1;32m    538\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:126\u001b[0m, in \u001b[0;36mvLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39mlast_line)]], llm_output\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mcontent))     \n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(p))] \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m prompts], llm_output\u001b[39m=\u001b[39m{})\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:126\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39mlast_line)]], llm_output\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mcontent))     \n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(p))] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts], llm_output\u001b[39m=\u001b[39m{})\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:94\u001b[0m, in \u001b[0;36mvLLM._call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m     92\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     93\u001b[0m use_beam_search \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost_http_request(prompt, api_url, stop, n, stream, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemperature, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_tokens, use_beam_search)\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_response(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Projets/OneOS/src/Console/client/chains/models/vllm.py:53\u001b[0m, in \u001b[0;36mvLLM.post_http_request\u001b[0;34m(self, prompt, api_url, stop, n, stream, temperature, max_tokens, use_beam_search)\u001b[0m\n\u001b[1;32m     43\u001b[0m headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient_name}\n\u001b[1;32m     44\u001b[0m pload \u001b[39m=\u001b[39m {\n\u001b[1;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt,\n\u001b[1;32m     46\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop\n\u001b[1;32m     52\u001b[0m }\n\u001b[0;32m---> 53\u001b[0m response \u001b[39m=\u001b[39m post(api_url, headers\u001b[39m=\u001b[39;49mheaders, json\u001b[39m=\u001b[39;49mpload, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Projets/OneOS/src/.venv/lib/python3.11/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5085): Max retries exceeded with url: /generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8429644350>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "agent_executor({\"input\": \"hi, im Danny but you can call me Sir.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m'action'\n",
      " {\n",
      "  \"function\": \"Python\",\n",
      "  \"parameters\": \"import datetime\\nprint(datetime.datetime.now().strftime('%H:%M:%S'))\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What time is it?',\n",
       " 'history': 'Human: hi, im Danny but you can call me Sir.\\nAI:  {\\n  \"function\": \"Clear\",\\n  \"parameters\": []\\n}',\n",
       " 'output': ' {\\n  \"function\": \"Python\",\\n  \"parameters\": \"import datetime\\\\nprint(datetime.datetime.now().strftime(\\'%H:%M:%S\\'))\"\\n}'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor({\"input\": \"What time is it?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m'action'\n",
      " {\n",
      "  \"function\": \"Search\",\n",
      "  \"parameters\": {\n",
      "    \"input\": \"What is my name?\",\n",
      "    \"search_terms\": [\"my name\"]\n",
      "  }\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name?',\n",
       " 'history': 'Human: hi, im Danny but you can call me Sir.\\nAI:  {\\n  \"function\": \"Clear\",\\n  \"parameters\": []\\n}\\nHuman: What time is it?\\nAI:  {\\n  \"function\": \"Python\",\\n  \"parameters\": \"import datetime\\\\nprint(datetime.datetime.now().strftime(\\'%H:%M:%S\\'))\"\\n}',\n",
       " 'output': ' {\\n  \"function\": \"Search\",\\n  \"parameters\": {\\n    \"input\": \"What is my name?\",\\n    \"search_terms\": [\"my name\"]\\n  }\\n}'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor({\"input\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m'action'\n",
      " {\n",
      "  \"function\": \"Clear\",\n",
      "  \"parameters\": []\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' {\\n  \"function\": \"Clear\",\\n  \"parameters\": []\\n}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(input=\"Hello there!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
